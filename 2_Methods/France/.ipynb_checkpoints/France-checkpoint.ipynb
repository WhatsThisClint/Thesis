{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31d3ac9-5463-468f-b49e-21b3eb4e4ca2",
   "metadata": {},
   "source": [
    "# Code According to France's Methdodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6844eb-6dfd-479c-b94b-1b220c27c7b4",
   "metadata": {},
   "source": [
    "1) The tineframe is 12 years for 1 cycle and 36 years for pluriannual assessments\n",
    "2) The yearly minimum and average values are computed from monthly values. At least 80% data is required per year for the computation. \n",
    "3) Mann Kendall test is performed on the yearly average and minimum values and at least 80% data is required for the entire test for each site. Alpha or confidence lvl is 0.05 (alpha = 0.05 by default in the pymannkendall package)\n",
    "4) If >20% results show a falling trend, the status is bad for that unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f6704e-6bf8-4408-88e2-2d0a8b3405ed",
   "metadata": {},
   "source": [
    "## Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd2079a5-e86c-4cc2-969c-94a976b21999",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a date (YYYY-MM):  2000-01\n",
      "Select cycle type:\n",
      "1. One cycle\n",
      "2. Pluriannual cycle\n",
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groundwater Unit Status: Bad\n",
      "Debug: Process completed.\n",
      "CPU times: total: 1min 46s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pymannkendall as mk\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = 'groundwater_timeseries_data_Negative.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Get user input for date\n",
    "while True:\n",
    "    try:\n",
    "        user_date = input(\"Enter a date (YYYY-MM): \")\n",
    "        user_date = datetime.strptime(user_date, \"%Y-%m\")\n",
    "        break\n",
    "    except ValueError:\n",
    "        print(\"Invalid date format. Please use YYYY-MM format.\")\n",
    "\n",
    "# Get user input for cycle type\n",
    "while True:\n",
    "    try:\n",
    "        cycle_type = int(input(\"Select cycle type:\\n1. One cycle\\n2. Pluriannual cycle\\n\"))\n",
    "        if cycle_type in [1, 2]:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please select 1 or 2.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "# Calculate years based on cycle type\n",
    "if cycle_type == 1:\n",
    "    required_years = 12\n",
    "else:\n",
    "    required_years = 36\n",
    "\n",
    "# Calculate start date for data selection\n",
    "start_date = user_date - pd.DateOffset(years=required_years)\n",
    "\n",
    "# Check if the required years of data are available\n",
    "if start_date.year < df['date'].dt.year.min() or user_date.year > df['date'].dt.year.max():\n",
    "    print(f\"Data is not available for {required_years} years prior to the selected date.\")\n",
    "else:\n",
    "    # Create a new DataFrame with filtered data\n",
    "    filtered_df = df[(df['date'] >= start_date) & (df['date'] <= user_date)]\n",
    "\n",
    "    # Calculate yearly average and minimum for each site\n",
    "    results = []\n",
    "    skipped_years = []\n",
    "\n",
    "    for site in filtered_df['site'].unique():\n",
    "        site_data = filtered_df[filtered_df['site'] == site]\n",
    "        yearly_data = site_data.groupby(site_data['date'].dt.year).agg({'level': ['mean', 'min']})\n",
    "        yearly_data.columns = ['avg_level', 'min_level']\n",
    "        yearly_data = yearly_data[yearly_data.index.isin(range(start_date.year, user_date.year + 1))]\n",
    "\n",
    "        for year, row in yearly_data.iterrows():\n",
    "            if len(site_data[site_data['date'].dt.year == year]) >= 10:\n",
    "                results.append((site, year, row['min_level'], row['avg_level']))\n",
    "            else:\n",
    "                skipped_years.append(year)\n",
    "\n",
    "    # Create a DataFrame with results\n",
    "    result_columns = ['site', 'year', 'min_level', 'avg_level']\n",
    "    result_df = pd.DataFrame(results, columns=result_columns)\n",
    "\n",
    "    # Check for sites with at least 80% data coverage\n",
    "    valid_sites = []\n",
    "    discarded_sites = []\n",
    "\n",
    "    for site in result_df['site'].unique():\n",
    "        site_data = result_df[result_df['site'] == site]\n",
    "        avg_level_coverage = len(site_data.dropna(subset=['avg_level'])) / required_years\n",
    "        min_level_coverage = len(site_data.dropna(subset=['min_level'])) / required_years\n",
    "        if avg_level_coverage >= 0.8 and min_level_coverage >= 0.8:\n",
    "            valid_sites.append(site)\n",
    "        else:\n",
    "            discarded_sites.append(site)\n",
    "\n",
    "    valid_result_df = result_df[result_df['site'].isin(valid_sites)]\n",
    "\n",
    "    # Save the results to an Excel file\n",
    "    output_file_path = 'groundwater_assessment_results.xlsx'\n",
    "    with pd.ExcelWriter(output_file_path) as writer:\n",
    "        valid_result_df.to_excel(writer, sheet_name='Results', index=False)\n",
    "        skipped_df = pd.DataFrame({'Skipped Years': skipped_years})\n",
    "        skipped_df.to_excel(writer, sheet_name='Skipped Years', index=False)\n",
    "        discarded_sites_df = pd.DataFrame({'Discarded Sites': discarded_sites})\n",
    "        discarded_sites_df.to_excel(writer, sheet_name='Discarded Sites', index=False)\n",
    "\n",
    "        # Perform Mann-Kendall analysis\n",
    "        mk_results = []\n",
    "        for site in valid_result_df['site'].unique():\n",
    "            site_data = valid_result_df[valid_result_df['site'] == site]\n",
    "            for col in ['avg_level', 'min_level']:\n",
    "                mk_test = mk.original_test(site_data[col])\n",
    "                mk_results.append((site, col, mk_test.trend, mk_test.h,\n",
    "                                   mk_test.p, mk_test.z, mk_test.Tau, mk_test.s, mk_test.var_s,\n",
    "                                   mk_test.slope, mk_test.intercept))\n",
    "\n",
    "        mk_columns = ['site', 'data_type', 'trend', 'h', 'p', 'z', 'Tau', 's', 'var_s', 'slope', 'intercept']\n",
    "        mk_result_df = pd.DataFrame(mk_results, columns=mk_columns)\n",
    "        mk_result_df.to_excel(writer, sheet_name='Mann-Kendall Results', index=False)\n",
    "\n",
    "        # Check for bad status\n",
    "        avg_level_trends = mk_result_df[mk_result_df['data_type'] == 'avg_level']['trend']\n",
    "        bad_status_threshold = 0.2 * len(avg_level_trends)\n",
    "        if sum(avg_level_trends == \"decreasing\") > bad_status_threshold:\n",
    "            print(\"Groundwater Unit Status: Bad :(\")\n",
    "\n",
    "    # Save the Mann-Kendall results to a CSV file\n",
    "    mk_result_csv_path = 'mann_kendall_results.csv'\n",
    "    mk_result_df.to_csv(mk_result_csv_path, index=False)\n",
    "\n",
    "    print(\"Process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff73945-a0d3-4ebd-b95b-cdd68c9665e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121f1663-3cae-40f7-b01b-fe60ece7888d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a date (YYYY-MM):  2022-01\n",
      "Select cycle type:\n",
      "1. One cycle\n",
      "2. Pluriannual cycle\n",
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groundwater Unit Status: Bad\n",
      "Debug: Process completed.\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pymannkendall as mk\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = 'groundwater_timeseries_data_Negative.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Get user input for date\n",
    "while True:\n",
    "    try:\n",
    "        user_date = input(\"Enter a date (YYYY-MM): \")\n",
    "        user_date = datetime.strptime(user_date, \"%Y-%m\")\n",
    "        break\n",
    "    except ValueError:\n",
    "        print(\"Invalid date format. Please use YYYY-MM format.\")\n",
    "\n",
    "# Get user input for cycle type\n",
    "while True:\n",
    "    try:\n",
    "        cycle_type = int(input(\"Select cycle type:\\n1. One cycle\\n2. Pluriannual cycle\\n\"))\n",
    "        if cycle_type in [1, 2]:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please select 1 or 2.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "# Calculate years based on cycle type\n",
    "if cycle_type == 1:\n",
    "    required_years = 12\n",
    "else:\n",
    "    required_years = 36\n",
    "\n",
    "# Calculate start date for data selection\n",
    "start_date = user_date - pd.DateOffset(years=required_years)\n",
    "\n",
    "# Check if the required years of data are available\n",
    "if start_date.year < df['date'].dt.year.min() or user_date.year > df['date'].dt.year.max():\n",
    "    print(f\"Data is not available for {required_years} years prior to the selected date.\")\n",
    "else:\n",
    "    # Create a new DataFrame with filtered data\n",
    "    filtered_df = df[(df['date'] >= start_date) & (df['date'] <= user_date)]\n",
    "\n",
    "    # Calculate yearly average and minimum for each site\n",
    "    results = []\n",
    "    skipped_years = []\n",
    "\n",
    "    for site in filtered_df['site'].unique():\n",
    "        site_data = filtered_df[filtered_df['site'] == site]\n",
    "        yearly_data = site_data.groupby(site_data['date'].dt.year).agg({'level': ['mean', 'min']})\n",
    "        yearly_data.columns = ['avg_level', 'min_level']\n",
    "        yearly_data = yearly_data[yearly_data.index.isin(range(start_date.year, user_date.year + 1))]\n",
    "\n",
    "        for year, row in yearly_data.iterrows():\n",
    "            if len(site_data[site_data['date'].dt.year == year]) >= 10:\n",
    "                results.append((site, year, row['min_level'], row['avg_level']))\n",
    "            else:\n",
    "                skipped_years.append(year)\n",
    "\n",
    "    # Create a DataFrame with results\n",
    "    result_columns = ['site', 'year', 'min_level', 'avg_level']\n",
    "    result_df = pd.DataFrame(results, columns=result_columns)\n",
    "\n",
    "    # Save the results to an Excel file\n",
    "    output_file_path = 'groundwater_assessment_results.xlsx'\n",
    "    with pd.ExcelWriter(output_file_path) as writer:\n",
    "        result_df.to_excel(writer, sheet_name='Results', index=False)\n",
    "        skipped_df = pd.DataFrame({'Skipped Years': skipped_years})\n",
    "        skipped_df.to_excel(writer, sheet_name='Skipped Years', index=False)\n",
    "\n",
    "        # Perform Mann-Kendall analysis\n",
    "        mk_results = []\n",
    "        for site in result_df['site'].unique():\n",
    "            site_data = result_df[result_df['site'] == site]\n",
    "            for col in ['avg_level', 'min_level']:\n",
    "                mk_test = mk.original_test(site_data[col])\n",
    "                mk_results.append((site, col, mk_test.trend, mk_test.h,\n",
    "                                   mk_test.p, mk_test.z, mk_test.Tau, mk_test.s, mk_test.var_s,\n",
    "                                   mk_test.slope, mk_test.intercept))\n",
    "\n",
    "        mk_columns = ['site', 'data_type', 'trend', 'h', 'p', 'z', 'Tau', 's', 'var_s', 'slope', 'intercept']\n",
    "        mk_result_df = pd.DataFrame(mk_results, columns=mk_columns)\n",
    "        mk_result_df.to_excel(writer, sheet_name='Mann-Kendall Results', index=False)\n",
    "\n",
    "        # Check for bad status\n",
    "        avg_level_trends = mk_result_df[mk_result_df['data_type'] == 'avg_level']['trend']\n",
    "        bad_status_threshold = 0.2 * len(avg_level_trends)\n",
    "        if sum(avg_level_trends == \"decreasing\") > bad_status_threshold:\n",
    "            print(\"Groundwater Unit Status: Bad\")\n",
    "\n",
    "    # Save the Mann-Kendall results to a CSV file\n",
    "    mk_result_csv_path = 'mann_kendall_results.csv'\n",
    "    mk_result_df.to_csv(mk_result_csv_path, index=False)\n",
    "\n",
    "    print(\"Saving Process Completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42ac27-bb86-433e-8eb9-0dc10bcf547d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pymannkendall as mk\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "filename = 'groundwater_timeseries_data_Negative.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Convert the 'date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Ask the user to select a date\n",
    "while True:\n",
    "    selected_date_str = input(\"Please enter a date (YYYY-MM-DD): \")\n",
    "    try:\n",
    "        selected_date = pd.to_datetime(selected_date_str)\n",
    "        break\n",
    "    except ValueError:\n",
    "        print(\"Invalid date format. Please enter the date in YYYY-MM-DD format.\")\n",
    "\n",
    "\n",
    "# Calculate the start and end dates for the 6-year reporting cycle (previous and next)\n",
    "\n",
    "one_cycle = selected_date - pd.DateOffset(years=12)\n",
    "Pluriannual_cycle = selected_date - pd.DateOffset(years=36)\n",
    "\n",
    "# Find the first and last dates in the data\n",
    "first_date_in_data = df['date'].min()\n",
    "last_date_in_data = df['date'].max()\n",
    "\n",
    "# Check if the selected date and reporting cycle exist within the available data\n",
    "if Pluriannual_cycle < first_date_in_data or end_date > last_date_in_data:\n",
    "    print(\"The analysis cannot be completed using this date. There needs to be a 6-year timeseries before and after the selected date. Please select a data accordingly.\")\n",
    "else:\n",
    "    # Filter the DataFrame to the selected date and the previous 6 years\n",
    "    two_cycle_df_ini = df[(df['date'] >= start_date_2_cycles) & (df['date'] <= end_date)]\n",
    "\n",
    "    # Filter the DataFrame to the selected date and the next 6 years\n",
    "    one_cycle_df_ini = df[(df['date'] >= start_date_1_cycle) & (df['date'] <= end_date)]\n",
    "        \n",
    "    # Remove rows where level values are 0\n",
    "    two_cycle_df = two_cycle_df_ini[two_cycle_df_ini['level'] != 0]\n",
    "    one_cycle_df = one_cycle_df_ini[one_cycle_df_ini['level'] != 0] \n",
    "    \n",
    "    # Count the number of months in one_cycle_df_ini and one_cycle_df\n",
    "    total_months_one_cycle_ini = len(one_cycle_df_ini['date'].dt.to_period('M').unique())\n",
    "    total_months_one_cycle = len(one_cycle_df['date'].dt.to_period('M').unique())\n",
    "\n",
    "    # Check if the number of months in both dataframes is >= one-third of the number of months in one_cycle_df_ini\n",
    "    if total_months_one_cycle >= total_months_one_cycle_ini / 3:\n",
    "        # Proceed with the analysis\n",
    "        \n",
    "        # Calculate the average level for each month and site for 2 cycles\n",
    "        avg_two_cycle_df = two_cycle_df.groupby(['date', 'site'])['level'].mean().reset_index()\n",
    "        \n",
    "        # Calculate the average level for each month and site for 1 cycle\n",
    "        avg_one_cycle_df = one_cycle_df.groupby(['date', 'site'])['level'].mean().reset_index()\n",
    "        \n",
    "        # Initialize counters for total sites \n",
    "        total_sites_one_cycle = len(avg_one_cycle_df['site'].unique())\n",
    "        total_sites_two_cycles = len(avg_one_cycle_df['site'].unique())\n",
    "        \n",
    "        results_one_cycle = []\n",
    "        data = avg_one_cycle_df \n",
    "        for site, site_data in data.groupby(\"site\"):\n",
    "            trend_result = mk.original_test(site_data[\"level\"])\n",
    "            results_one_cycle.append({\n",
    "                \"site\": site,\n",
    "                \"trend\": trend_result.trend,\n",
    "                \"h\": trend_result.h,\n",
    "                \"p\": trend_result.p,\n",
    "                \"z\": trend_result.z,\n",
    "                \"Tau\": trend_result.Tau,\n",
    "                \"s\": trend_result.s,\n",
    "                \"var_s\": trend_result.var_s,\n",
    "                \"slope\": trend_result.slope,\n",
    "                \"intercept\": trend_result.intercept\n",
    "            })\n",
    "            # Create a new DataFrame with results\n",
    "        results_one_cycle_df = pd.DataFrame(results_one_cycle)\n",
    "\n",
    "        # Write the results to a new CSV file\n",
    "        output_file = \"Cycle_one_results.csv\"\n",
    "        results_one_cycle_df.to_csv(output_file, index=False)      \n",
    "        \n",
    "        results_two_cycle = []\n",
    "        data = avg_two_cycle_df \n",
    "        for site, site_data in data.groupby(\"site\"):\n",
    "            trend_result = mk.original_test(site_data[\"level\"])\n",
    "            results_two_cycle.append({\n",
    "                \"site\": site,\n",
    "                \"trend\": trend_result.trend,\n",
    "                \"h\": trend_result.h,\n",
    "                \"p\": trend_result.p,\n",
    "                \"z\": trend_result.z,\n",
    "                \"Tau\": trend_result.Tau,\n",
    "                \"s\": trend_result.s,\n",
    "                \"var_s\": trend_result.var_s,\n",
    "                \"slope\": trend_result.slope,\n",
    "                \"intercept\": trend_result.intercept\n",
    "            })\n",
    "            # Create a new DataFrame with results\n",
    "        results_two_cycle_df = pd.DataFrame(results_two_cycle)\n",
    "\n",
    "        # Write the results to a new CSV file\n",
    "        output_file = \"Cycle_two_results.csv\"\n",
    "        results_two_cycle_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        # Calculate the number of rows where the trend is \"decreasing\" for one cycle\n",
    "        decreasing_sites_one_cycle = len(results_one_cycle_df[results_one_cycle_df['trend'] == 'decreasing'])\n",
    "\n",
    "        # Calculate the percentage\n",
    "        percentage_decreasing_one_cycle = (decreasing_sites_one_cycle / total_sites_one_cycle) * 100\n",
    "\n",
    "        print(f\"Percentage of sites with decreasing trend for one cycle is: {percentage_decreasing_one_cycle:.2f}%\")\n",
    "               \n",
    "        # Calculate the number of rows where the trend is \"decreasing\" for two cycles\n",
    "        decreasing_sites_two_cycles = len(results_two_cycle_df[results_two_cycle_df['trend'] == 'decreasing'])\n",
    "\n",
    "        # Calculate the percentage\n",
    "        percentage_decreasing_two_cycles = (decreasing_sites_two_cycles / total_sites_two_cycles) * 100\n",
    "\n",
    "        print(f\"Percentage of sites with decreasing trend for two cycles are: {percentage_decreasing_two_cycles:.2f}%\")\n",
    "        # Check if the percentage of decreasing sites is greater than 20%\n",
    "        if percentage_decreasing_two_cycles > 20:\n",
    "            print(\"The status of the aquifer is bad :(.\")\n",
    "        else:\n",
    "            print(\"The status of the aquifer is good yay!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"The analysis cannot be performed due to the lack of data for the selected reporting cycle.\")\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
